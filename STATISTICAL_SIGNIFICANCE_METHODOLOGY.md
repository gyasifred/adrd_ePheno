# Statistical Significance Testing for ADRD ePhenotyping

**Purpose**: This document explains statistical methods for evaluating performance differences across demographic subgroups in the ADRD ePhenotyping project (Aim 1).

**Status**: Methodology document - NOT YET IMPLEMENTED IN CODE

**Date**: 2025-11-06

---

## Table of Contents

1. [Overview](#overview)
2. [Approximate Randomization Testing](#approximate-randomization-testing)
3. [Bootstrap Confidence Intervals](#bootstrap-confidence-intervals)
4. [Multiple Testing Correction](#multiple-testing-correction)
5. [Effect Size Measures](#effect-size-measures)
6. [Implementation Guidelines](#implementation-guidelines)
7. [R Package Recommendations](#r-package-recommendations)
8. [Example Code](#example-code)
9. [Interpretation Guidelines](#interpretation-guidelines)

---

## Overview

### The Challenge

When comparing model performance across multiple demographic subgroups, we face several statistical challenges:

1. **Multiple comparisons**: Testing many subgroups increases false positive rate
2. **Correlated metrics**: AUC, sensitivity, specificity are not independent
3. **Imbalanced groups**: Some demographic categories have few samples
4. **Non-normal distributions**: Performance metrics may not follow normal distributions

### Our Approach

We use a combination of methods to address these challenges:

- **Primary**: Approximate randomization testing (permutation tests)
- **Confidence intervals**: Bootstrap methods for robust CI estimation
- **Multiple testing correction**: FDR control via Benjamini-Hochberg
- **Effect sizes**: To distinguish statistical vs. practical significance

---

## Approximate Randomization Testing

### What is it?

**Approximate randomization** (also called **permutation testing**) is a non-parametric method for testing hypotheses by comparing observed test statistics to a distribution generated by randomly permuting group labels.

### Why use it?

✅ **Advantages**:
- No assumptions about distribution (non-parametric)
- Works with small sample sizes
- Exact p-values under the null hypothesis
- Can be used with any test statistic (AUC, sensitivity, F1, etc.)
- Naturally handles correlated predictors

❌ **Limitations**:
- Computationally intensive (requires many permutations)
- May be conservative with very small samples
- Assumes exchangeability under null hypothesis

### Methodology

**Null Hypothesis (H₀)**: There is no difference in model performance between groups A and B

**Test Procedure**:

1. **Calculate observed statistic**: Compute the difference in performance metric between groups
   ```
   observed_diff = metric_A - metric_B
   ```

2. **Generate permutation distribution**:
   - For i = 1 to N_permutations (typically 1000-10000):
     a. Randomly shuffle group labels
     b. Recalculate performance metrics for shuffled groups
     c. Compute difference: perm_diff[i] = shuffled_metric_A - shuffled_metric_B

3. **Calculate p-value**:
   ```
   p_value = (number of |perm_diff| >= |observed_diff|) / N_permutations
   ```

4. **Decision**: Reject H₀ if p_value < α (typically α = 0.05 after correction)

### Example Scenario

**Question**: Does the CNN model perform differently for Males vs Females?

- **Group A (Males)**: AUC = 0.85
- **Group B (Females)**: AUC = 0.88
- **Observed difference**: 0.03

**Permutation test**:
1. Pool all predictions (both males and females)
2. Randomly assign "male" or "female" labels maintaining original group sizes
3. Recalculate AUCs for permuted groups
4. Repeat 10,000 times
5. Count how many permutations produce |difference| ≥ 0.03
6. If only 200/10000 do → p-value = 0.02 → statistically significant

---

## Bootstrap Confidence Intervals

### What is it?

**Bootstrap** is a resampling method to estimate the sampling distribution of a statistic by repeatedly sampling with replacement from the observed data.

### Why use it?

✅ **Advantages**:
- No distributional assumptions
- Works with complex statistics (AUC, F1, etc.)
- Provides confidence intervals for effect sizes
- Can assess uncertainty in small samples

### Methodology

**Goal**: Construct 95% confidence interval for AUC (or other metric)

**Procedure**:

1. **Stratified bootstrap sampling**:
   - For i = 1 to N_bootstrap (typically 1000-10000):
     a. Sample with replacement from true positives (ADRD cases)
     b. Sample with replacement from true negatives (controls)
     c. Maintain original class proportions
     d. Calculate metric on bootstrapped sample: metric_boot[i]

2. **Construct CI**:
   - **Percentile method**: [quantile(metric_boot, 0.025), quantile(metric_boot, 0.975)]
   - **BCa method**: Bias-corrected and accelerated (more accurate, implemented in `boot` package)

3. **Interpretation**:
   - If CI for group A and CI for group B do not overlap → likely significant difference
   - If CI for difference (A-B) excludes 0 → significant difference

### Example

**Male group**:
- Observed AUC = 0.850
- Bootstrap 95% CI: [0.820, 0.875]

**Female group**:
- Observed AUC = 0.880
- Bootstrap 95% CI: [0.860, 0.900]

**Difference**:
- Observed: -0.030
- Bootstrap 95% CI for difference: [-0.065, -0.005]
- **Interpretation**: Significant difference (CI excludes 0)

---

## Multiple Testing Correction

### The Problem

When testing K hypotheses, the family-wise error rate (FWER) increases:

```
FWER = 1 - (1 - α)^K
```

For α = 0.05 and K = 10 tests:
```
FWER = 1 - (0.95)^10 = 0.40 (40% chance of at least one false positive!)
```

### Solution 1: Bonferroni Correction

**Most conservative** approach:

```
α_corrected = α / K
```

For α = 0.05 and K = 10 tests:
```
α_corrected = 0.05 / 10 = 0.005
```

✅ **Pros**: Simple, controls FWER
❌ **Cons**: Very conservative, low power, assumes independent tests

### Solution 2: Benjamini-Hochberg FDR Control (RECOMMENDED)

**Balances** false positives and false negatives by controlling **False Discovery Rate (FDR)**:

**FDR** = Expected proportion of false positives among rejected hypotheses

**Procedure**:
1. Perform all K tests, obtain p-values: p₁, p₂, ..., pₖ
2. Sort p-values: p₍₁₎ ≤ p₍₂₎ ≤ ... ≤ p₍ₖ₎
3. Find largest i such that: p₍ᵢ₎ ≤ (i/K) × α
4. Reject hypotheses 1, 2, ..., i

**Example**:
```
K = 5 comparisons, α = 0.05

Sorted p-values and thresholds:
Rank (i)  |  p-value  |  Threshold (i/5 × 0.05)  |  Decision
------------------------------------------------------------------------
   1      |  0.001    |  0.010                   |  Reject (0.001 < 0.010)
   2      |  0.015    |  0.020                   |  Reject (0.015 < 0.020)
   3      |  0.025    |  0.030                   |  Reject (0.025 < 0.030)
   4      |  0.035    |  0.040                   |  Reject (0.035 < 0.040)
   5      |  0.060    |  0.050                   |  Fail to reject

Result: Reject first 4 hypotheses
```

✅ **Pros**: Less conservative than Bonferroni, higher power, controls FDR
❌ **Cons**: Allows some false positives (by design)

**When to use**:
- ✅ Exploratory analysis (Aim 1 demographic comparisons)
- ✅ Many comparisons (>5 subgroups)
- ❌ Critical clinical decisions (use Bonferroni)

---

## Effect Size Measures

### Why Effect Sizes Matter

**Statistical significance ≠ Practical significance**

With large samples, tiny differences can be statistically significant but clinically meaningless.

**Solution**: Always report effect sizes alongside p-values

### Effect Sizes for Binary Outcomes

#### 1. Cohen's d (for continuous metrics like AUC)

**Definition**:
```
d = (mean_A - mean_B) / pooled_SD
```

**Interpretation** (Cohen's guidelines):
- Small: |d| = 0.20
- Medium: |d| = 0.50
- Large: |d| = 0.80

**Example**:
```
AUC_males = 0.850 (SD = 0.030)
AUC_females = 0.880 (SD = 0.025)

pooled_SD = sqrt((0.030² + 0.025²) / 2) = 0.0277

d = (0.850 - 0.880) / 0.0277 = -1.08  (Large effect)
```

#### 2. Absolute Difference in AUC

**Interpretation** (for clinical models):
- Negligible: |ΔAUC| < 0.01
- Small: 0.01 ≤ |ΔAUC| < 0.03
- Moderate: 0.03 ≤ |ΔAUC| < 0.05
- Large: |ΔAUC| ≥ 0.05

**Example**:
```
ΔAUC = 0.880 - 0.850 = 0.030  (Small-to-moderate clinical difference)
```

#### 3. Relative Risk (for classification outcomes)

**For sensitivity/specificity**:
```
RR = sensitivity_A / sensitivity_B
```

**Interpretation**:
- RR = 1.00: No difference
- RR = 1.10: 10% higher sensitivity in group A
- RR = 0.90: 10% lower sensitivity in group A

### Practical Significance Thresholds (Recommended for ADRD)

| Metric | Threshold for "Meaningful Difference" |
|--------|--------------------------------------|
| AUC | ΔAUC ≥ 0.03 |
| Sensitivity | Δ ≥ 0.05 (5 percentage points) |
| Specificity | Δ ≥ 0.05 (5 percentage points) |
| F1 Score | ΔF1 ≥ 0.03 |
| PPV/NPV | Δ ≥ 0.05 |

**Rationale**: These thresholds balance clinical relevance with achievable improvements

---

## Implementation Guidelines

### Step-by-Step Workflow for Demographic Comparisons

#### Phase 1: Descriptive Analysis
1. Calculate performance metrics for each subgroup
2. Create visualization (bar plots with error bars)
3. Document sample sizes and class distributions

#### Phase 2: Pairwise Comparisons (if 2 groups)
1. **Calculate effect size** (Cohen's d, ΔAUC)
2. **Permutation test** (10,000 permutations)
3. **Bootstrap CI** for difference (10,000 bootstrap samples)
4. Report: effect size, p-value, 95% CI

#### Phase 3: Multiple Group Comparisons (if >2 groups)
1. **Global test**: Kruskal-Wallis or permutation ANOVA
2. If global test significant:
   - Perform pairwise comparisons
   - Apply Benjamini-Hochberg FDR correction
3. Report: adjusted p-values, effect sizes

#### Phase 4: Interpretation
1. Consider both statistical AND practical significance
2. Check for confounding (e.g., age differences between groups)
3. Assess clinical relevance of observed differences

---

## R Package Recommendations

### Core Packages

#### 1. `pROC` (already installed)
- Calculate AUC with confidence intervals
- Compare correlated ROC curves
- De Long's test for AUC differences

```r
library(pROC)

roc_male <- roc(labels_male, predictions_male)
roc_female <- roc(labels_female, predictions_female)

# Compare AUCs
roc.test(roc_male, roc_female, method = "delong")
```

#### 2. `coin` (permutation tests)
- Non-parametric permutation framework
- Handles various test statistics
- Stratification support

```r
library(coin)

# Permutation test for AUC difference
independence_test(AUC ~ Group, data = perf_data,
                  teststat = "maximum",
                  distribution = approximate(nresample = 10000))
```

#### 3. `boot` (bootstrap)
- Comprehensive bootstrap framework
- BCa confidence intervals
- Stratified sampling

```r
library(boot)

# Bootstrap CI for AUC
boot_auc <- boot(data, statistic = auc_function, R = 10000,
                 strata = data$label)  # Stratify by outcome
boot.ci(boot_auc, type = "bca")
```

#### 4. `lmPerm` (permutation ANOVA)
- Permutation-based ANOVA
- No normality assumptions

```r
library(lmPerm)

# Permutation ANOVA
aovp(AUC ~ DemographicGroup, data = results, perm = "Exact")
```

### Utility Packages

#### 5. `effsize` (effect sizes)
```r
library(effsize)

cohen.d(auc_group_a, auc_group_b)
```

#### 6. `multcomp` (multiple comparisons)
```r
library(multcomp)

# Adjust p-values
p.adjust(p_values, method = "BH")  # Benjamini-Hochberg
```

---

## Example Code

### Example 1: Permutation Test for AUC Difference

```r
# Function to perform permutation test for AUC difference
permutation_test_auc <- function(labels_a, pred_a, labels_b, pred_b,
                                  n_perm = 10000, seed = 42) {
  library(pROC)

  set.seed(seed)

  # Calculate observed AUCs
  auc_a_obs <- auc(roc(labels_a, pred_a, quiet = TRUE))
  auc_b_obs <- auc(roc(labels_b, pred_b, quiet = TRUE))
  observed_diff <- auc_a_obs - auc_b_obs

  # Pool data
  labels_pooled <- c(labels_a, labels_b)
  pred_pooled <- c(pred_a, pred_b)
  n_a <- length(labels_a)
  n_b <- length(labels_b)
  n_total <- n_a + n_b

  # Permutation distribution
  perm_diffs <- numeric(n_perm)

  for (i in seq_len(n_perm)) {
    # Shuffle indices
    shuffled_idx <- sample(n_total)

    # Split into groups maintaining original sizes
    idx_a <- shuffled_idx[1:n_a]
    idx_b <- shuffled_idx[(n_a + 1):n_total]

    # Calculate AUCs
    auc_a_perm <- auc(roc(labels_pooled[idx_a], pred_pooled[idx_a],
                          quiet = TRUE))
    auc_b_perm <- auc(roc(labels_pooled[idx_b], pred_pooled[idx_b],
                          quiet = TRUE))

    perm_diffs[i] <- auc_a_perm - auc_b_perm
  }

  # Calculate p-value (two-sided)
  p_value <- mean(abs(perm_diffs) >= abs(observed_diff))

  return(list(
    observed_diff = observed_diff,
    auc_a = auc_a_obs,
    auc_b = auc_b_obs,
    p_value = p_value,
    perm_diffs = perm_diffs
  ))
}

# Usage
result <- permutation_test_auc(
  labels_a = test_data$label[test_data$gender == "Male"],
  pred_a = test_data$pred[test_data$gender == "Male"],
  labels_b = test_data$label[test_data$gender == "Female"],
  pred_b = test_data$pred[test_data$gender == "Female"],
  n_perm = 10000
)

cat("AUC Male:", round(result$auc_a, 4), "\n")
cat("AUC Female:", round(result$auc_b, 4), "\n")
cat("Difference:", round(result$observed_diff, 4), "\n")
cat("P-value:", result$p_value, "\n")
```

### Example 2: Bootstrap Confidence Interval for AUC

```r
# Function to bootstrap CI for AUC difference
bootstrap_auc_ci <- function(labels, predictions, groups, n_boot = 10000,
                             conf_level = 0.95, seed = 42) {
  library(pROC)
  library(boot)

  set.seed(seed)

  # Stratified bootstrap function
  auc_diff_boot <- function(data, indices) {
    # Resample within each stratum (outcome class)
    boot_data <- data[indices, ]

    auc_a <- auc(roc(boot_data$label[boot_data$group == levels(data$group)[1]],
                     boot_data$pred[boot_data$group == levels(data$group)[1]],
                     quiet = TRUE))

    auc_b <- auc(roc(boot_data$label[boot_data$group == levels(data$group)[2]],
                     boot_data$pred[boot_data$group == levels(data$group)[2]],
                     quiet = TRUE))

    return(auc_a - auc_b)
  }

  # Prepare data
  data_df <- data.frame(
    label = labels,
    pred = predictions,
    group = factor(groups)
  )

  # Bootstrap with stratification
  boot_results <- boot(
    data = data_df,
    statistic = auc_diff_boot,
    R = n_boot,
    strata = data_df$label  # Stratify by outcome
  )

  # Calculate CI
  ci <- boot.ci(boot_results, conf = conf_level, type = "bca")

  return(list(
    observed = boot_results$t0,
    boot_dist = boot_results$t,
    ci_lower = ci$bca[4],
    ci_upper = ci$bca[5],
    boot_results = boot_results
  ))
}

# Usage
ci_result <- bootstrap_auc_ci(
  labels = test_data$label,
  predictions = test_data$pred,
  groups = test_data$gender,
  n_boot = 10000
)

cat("AUC Difference:", round(ci_result$observed, 4), "\n")
cat("95% CI: [", round(ci_result$ci_lower, 4), ", ",
    round(ci_result$ci_upper, 4), "]\n", sep = "")
```

### Example 3: Multiple Testing Correction

```r
# Example: Compare performance across 5 racial groups
# Perform all pairwise comparisons and correct for multiple testing

racial_groups <- c("White", "Black", "Asian", "Hispanic", "Other")
n_groups <- length(racial_groups)
n_comparisons <- choose(n_groups, 2)  # 10 pairwise comparisons

# Store results
comparison_results <- data.frame(
  group_a = character(),
  group_b = character(),
  auc_a = numeric(),
  auc_b = numeric(),
  diff = numeric(),
  p_value = numeric(),
  stringsAsFactors = FALSE
)

# Perform all pairwise comparisons
for (i in seq_len(n_groups - 1)) {
  for (j in (i + 1):n_groups) {
    group_a <- racial_groups[i]
    group_b <- racial_groups[j]

    # Get data for these groups
    data_a <- test_data[test_data$race == group_a, ]
    data_b <- test_data[test_data$race == group_b, ]

    # Permutation test
    perm_result <- permutation_test_auc(
      data_a$label, data_a$pred,
      data_b$label, data_b$pred,
      n_perm = 10000
    )

    # Store result
    comparison_results <- rbind(comparison_results, data.frame(
      group_a = group_a,
      group_b = group_b,
      auc_a = perm_result$auc_a,
      auc_b = perm_result$auc_b,
      diff = perm_result$observed_diff,
      p_value = perm_result$p_value
    ))
  }
}

# Apply Benjamini-Hochberg correction
comparison_results$p_adjusted <- p.adjust(comparison_results$p_value,
                                          method = "BH")

# Identify significant comparisons
comparison_results$significant <- comparison_results$p_adjusted < 0.05

# Display results
print(comparison_results)

# Summarize
cat("\n", sum(comparison_results$significant), " significant comparisons out of ",
    nrow(comparison_results), " after FDR correction\n", sep = "")
```

---

## Interpretation Guidelines

### Decision Framework

For each demographic comparison:

1. **Calculate metrics**:
   - Observed difference (e.g., ΔAUC)
   - Effect size (Cohen's d)
   - P-value (permutation test)
   - 95% CI (bootstrap)

2. **Assess statistical significance**:
   - Is p-value < 0.05 (after FDR correction)?
   - Does 95% CI exclude 0?

3. **Assess practical significance**:
   - Is |ΔAUC| ≥ 0.03?
   - Is Cohen's |d| ≥ 0.5?

4. **Decision matrix**:

| Statistical | Practical | Interpretation | Action |
|-------------|-----------|----------------|--------|
| Yes | Yes | **Meaningful difference** | Investigate cause, consider intervention |
| Yes | No | **Significant but small** | Monitor, may not require action |
| No | Yes | **Trend worth monitoring** | Collect more data |
| No | No | **No meaningful difference** | No action needed |

### Reporting Template

**Example Write-up**:

> Model performance was compared between male and female patients using permutation testing (10,000 permutations). Males demonstrated significantly lower AUC compared to females (0.850 vs 0.880, difference = -0.030, p = 0.012, 95% CI [-0.052, -0.008], Cohen's d = -1.08). This difference is both statistically significant (p < 0.05) and clinically meaningful (|ΔAUC| ≥ 0.03), suggesting potential gender bias in the model that warrants further investigation.

---

## Frequently Asked Questions

### Q1: How many permutations do I need?

**Answer**:
- **Minimum**: 1,000 for α = 0.05
- **Recommended**: 10,000 for stable p-values
- **High precision**: 100,000 if p-value is very small or needs reporting to 3+ decimals

**Rule**: Use at least 1/α permutations (e.g., 20,000 for α = 0.05)

### Q2: Should I use one-tailed or two-tailed tests?

**Answer**: **Two-tailed** (default) unless you have strong a priori hypothesis about direction

**Reason**: We want to detect bias in either direction (underperformance OR overperformance for a group)

### Q3: What if sample sizes are very unbalanced?

**Example**: 500 White patients, 20 Black patients

**Answer**:
- Bootstrap/permutation tests handle this naturally
- **BUT**: Small groups have wider confidence intervals
- **Recommendation**:
  - Report metrics but flag small sample size
  - Use MIN_SUBGROUP_SIZE = 10 threshold
  - Consider collapsing rare categories

### Q4: How do I handle missing demographic data?

**Options**:
1. **Complete case analysis**: Exclude patients with missing demographics
   - Pros: Clean analysis
   - Cons: May introduce bias, reduces sample size

2. **"Unknown" category**: Treat missing as separate group
   - Pros: Retains all data
   - Cons: "Unknown" may be heterogeneous

3. **Multiple imputation**: Impute missing values (advanced)
   - Pros: Unbiased estimates if done correctly
   - Cons: Complex, requires auxiliary variables

**Recommendation for this project**: Option 1 (complete case), document exclusions

### Q5: What if I find significant differences?

**Follow-up analyses**:
1. **Investigate data quality**: Are there documentation differences?
2. **Check for confounders**: Age, comorbidities, care settings
3. **Examine predictions**: Are certain phrases driving the difference?
4. **Consider interventions**:
   - Retraining with balanced data
   - Post-hoc calibration by subgroup
   - Separate models for subgroups

---

## Summary Checklist

For each demographic comparison, ensure you:

- [ ] Calculate descriptive statistics (N, prevalence by group)
- [ ] Calculate performance metrics (AUC, sensitivity, specificity, F1)
- [ ] Compute effect size (Cohen's d, ΔAUC)
- [ ] Run permutation test (≥10,000 permutations)
- [ ] Calculate bootstrap CI (≥10,000 bootstrap samples)
- [ ] Apply multiple testing correction (if >1 comparison)
- [ ] Assess both statistical AND practical significance
- [ ] Create visualization (bar plot, forest plot, or scatter)
- [ ] Document all methods and parameters
- [ ] Report: effect size, p-value, CI, sample sizes

---

## References

### Key Papers

1. **Permutation Tests**: Good, P. (2005). *Permutation, Parametric, and Bootstrap Tests of Hypotheses*. Springer.

2. **Bootstrap Methods**: Efron, B., & Tibshirani, R. J. (1994). *An Introduction to the Bootstrap*. CRC Press.

3. **FDR Control**: Benjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. *Journal of the Royal Statistical Society: Series B*, 57(1), 289-300.

4. **Effect Sizes**: Cohen, J. (1988). *Statistical Power Analysis for the Behavioral Sciences* (2nd ed.). Routledge.

5. **ROC Comparison**: DeLong, E. R., DeLong, D. M., & Clarke-Pearson, D. L. (1988). Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach. *Biometrics*, 837-845.

6. **ML Fairness**: Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2021). A survey on bias and fairness in machine learning. *ACM Computing Surveys*, 54(6), 1-35.

### R Documentation

- `pROC` package: https://cran.r-project.org/package=pROC
- `coin` package: https://cran.r-project.org/package=coin
- `boot` package: https://cran.r-project.org/package=boot
- `effsize` package: https://cran.r-project.org/package=effsize

---

## Implementation Timeline

**Phase 1** (Week 1): Descriptive analysis
- Calculate metrics by subgroup
- Create visualizations
- Identify groups with sufficient sample size

**Phase 2** (Week 2): Statistical testing
- Implement permutation tests
- Calculate bootstrap CIs
- Apply multiple testing correction

**Phase 3** (Week 3): Reporting
- Interpret results
- Create summary tables
- Write methods section

---

## Document Status

**Current Status**: ✅ Methodology documented - Ready for implementation

**Next Steps**:
1. Implement helper functions in separate R script
2. Integrate into `04_demographic_analysis.R`
3. Validate with example data
4. Generate reports

**Maintainer**: Research Team
**Last Updated**: 2025-11-06

---

**END OF DOCUMENT**
